{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>\n",
    "# Prostate segmentation with V-Net\n",
    "\n",
    "#  <span style=\"color:green\">Overview</span>\n",
    "\n",
    "Being able to process volumetric data is a necessity in medical image analysis. Most medical data is in fact volumetric. CT scans, MRI, PET, SPECT and even ultrasound are producing 3D data which contain several million voxels (3D pixels) per scan. Although this type of data is extremely useful for diagnosis, as it reflects the true patient 3D anatomical configuration, dealing with such dimensionality can be challenging from a computational standpoint. In particular, neural networks which need to be trained on this kind of data suffer from the computational challenges brought by the amount of information in these scans. Recent advancements in both GPU computing capabilities, research [Ref. 1] and deep learning software have made the application of 3D-CNNs to 3D data possible. Large GPU memory sizes, the availability of accelerated software routines for 3D convolution, deconvolution and pooling as well as theoretical imporvements such as the introduction of Dice loss, group normalization, skip connections and encoder-decoder architectures have enabled interesting advancements in this field. \n",
    "\n",
    "In this exercise we show how to implement a popular volumetric CNN design, similar to V-Net [Ref. 1] and train it on 3D MRI data depicting prostate obtained from the segmentation decathlon 2018 challenge (http://medicaldecathlon.com). Our implementation is in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:green\">Introduction</span>\n",
    "\n",
    "V-Net has introduced three important novelties in DL medical image processing.\n",
    "* 3D network architecture processing volumes natively and at high resolution\n",
    "* Dice loss layer to learn how to segment without suffering from the drawbacks of other losses\n",
    "* (Long +) Short skip connections to accelerate learning and convergence.\n",
    "\n",
    "## Dice loss for segmentation\n",
    "The dice coefficient measures the overlap between two (binary) contours and has been generalized and introduced as an objective function for FCNNs in [Ref. 3]. Since then it has been utilized in a number of scientific works and it is now very well established. The formulation used in this work is DICE=2 * (Gt * Pred) / (Gt^2 + Pred^2). This corresponds to Dice when both Gt and Pred are binary.\n",
    "\n",
    "other formulations such as DICE=2 * (Gt * Pred) / (Gt + Pred)\n",
    "\n",
    "have been proposed but have slightly different behaviours, especially when it comes to gradients. You can get more information about this topic in [Ref. 5]\n",
    "\n",
    "## The importance of skip connections in biomedical image segmentation\n",
    "In a medical segmentation task, we want to map every voxel location of a medical image to a distinct class value representing for example background or organ of interest. Consequently, the input (image) and the output (segmentation) usually have the same spatial extent. A straight-forward approach to design a neural network architecture to achieve this goal would be to have several fully connected layers without changing the spatial dimensionality. However, this approach would very quickly lead to an explosion in terms of number of parameters. Convolutional operations have the advantage to significantly reduce the number of parameters, as the same operation is applied in strides over the entire image. Furthermore they are invariant to translation, e.g. it is not important if the object of interest is shifted. \n",
    "\n",
    "Designing a fully convolutional network without reducing the spatial dimensions would be possible, but research indicates that a encoder-decoder network is more effective [Ref. 6]. An encoder-decoder network typically looks like this:\n",
    "\n",
    "![Encoder-Decoder Network](encoder.png)\n",
    "\n",
    "The contracting path (Encoder) maps the image to a feature representation that is often lower dimensional than the original input size. The expanding path (Decoder) maps the feature representation to the output space. Originally, this type of network was known in connection with auto-encoder, where the output space is the same as the input space. In case of segmentation the output space often has the same spatial extend as the input space, but represents different content (classes). \n",
    "\n",
    "Thanks to the downsampling in the contracting path, fewer parameters need to be trained. However, this comes at the cost of losing spatial information. One approach to counteract this is the use of so-called skip connections: allowing the gradient to skip part of the network and to flow directly from a layer of the contracting part to the expanding path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusing features from different layers\n",
    "There are different techniques to realize these skip connections. Two of the most common ones are concatenation and summation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature concatenation\n",
    "One possibility is to simply concatenate the layers. This requires the layers to have the same dimension in the concatenation direction. \n",
    "So a concatenation t1 = [1 2 3] with t2 = [4 5 6] could be t_new = [1 2 3 4 5 6].  In Pytorch this operation is torch.cat ([see here](https://pytorch.org/docs/stable/torch.html)).\n",
    "\n",
    "<img src=concatenation.png width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature summation\n",
    "Another widely used approach is element-wise summation. One very nice property is that it keeps the number of features fixed. A summation of t1= [1 2 3] and t2 = [4 5 6] would be t_new = [5 7 9]. \n",
    "<img src=summation.png width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:green\">Best practice for structuring code</span>\n",
    "\n",
    "## Code and exercise structure\n",
    "Structuring code to solve a machine learning problem to ensure both flexibility and adoption of best practices is not an easy task. In this exercise we try to incorporate some of the best principles that have emerged in popular recent python projects.\n",
    "\n",
    "With the introduction of modern frameworks such as tensorflow and pytorch, most of the processes around developement of DL approaches have been standardized. During developement of a typical project it is necessary to take care of only a handful of compartimentalized tasks such as:\n",
    "* DATA\n",
    "    1. Load data, standardize and augment it\n",
    "    2. Split dataset into batches and iterate through them\n",
    "* NETWORK\n",
    "    1. Define a network architecture as computational graph\n",
    "    2. Define suitable loss\n",
    "* OPTIMIZATION\n",
    "    1. Define optimization algorithm\n",
    "    2. Implement training and validation loops\n",
    "    \n",
    "### Handling Data\n",
    "\n",
    "For **data** handling we define transforms in charge of loading, standardizing and modifying the dataset. Our transforms are chainable (stackable) such that data handling pipelines can be created. The dataset is stored in a python **dictionary** in order to allow this behaviour.\n",
    "\n",
    "**Transforms** are implemented by classes. In the constructor of these transforms (`__init__` method in python) we pass the parameters of the transform. We define the `__call__` method to accept only one user defined argument which is the dictionary containing data. \n",
    "\n",
    "```\n",
    "class ExampleTransform(object):\n",
    "    # this transformation adds a constant to the images of a dataset\n",
    "    \n",
    "    def __init__(self, constant):\n",
    "        # the constant that we add is passed as a parameter of this transform \n",
    "        self.constant = constant\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        # data is a dictionary containing the dataset. we suppose it has a field 'images'\n",
    "        data['images'] = data['images'] + self.constant\n",
    "        \n",
    "        # the modified version of the data dictionary is returned as a result of the transform\n",
    "        return data \n",
    "```\n",
    "\n",
    "The example code above implements a simple transform that adds a constant to all of the images of the dataset. Other transforms, including those aiming at loading datasets from filesystem and actually inject new data into the 'data' dictionary can be implemented.\n",
    "\n",
    "In order to split the dataset into batches and iterate through these batches during training validation and potentially testing we need a batch iterator. This is implemented here through a python class which acts as a generator. That is, we can use our batch iterator object in a for loop to get batches that we can use during training. The batch iterator will return a dictionary containing data at each iteration. The batch iterator is also able to execute tranforms (as defined above) both before and during iterating over the dataset. More details will be shown later in the exercise. \n",
    "\n",
    "### Network definition\n",
    "\n",
    "In the following sections of this exercise you will find the implementation of the network object in pytorch. A few network block objects have been defined in order to break down the implementation in more manageable portions and group together code that can be re-used.\n",
    "\n",
    "Loss layers can be defined very easily in pytroch by implementing only the 'forward' computation of the loss and omitting the gradient implementation. This is possible thanks to the built-in automatic differentiation capabilities of pytorch and other modern DL frameworks.\n",
    "\n",
    "### Fitting the networks parameters to the data\n",
    "\n",
    "In order to train, validate, and test the network we need to write the relevant code implementing the training, validation and testing loops (testing loop omitted here). The basic functionality of this code is to instantiate the network layers (init), the network forward computation function (forward member function), and finally instantiate batch iterators and optimizer.  \n",
    "At this point we can iterate (using a for loop) through the batches which can be fed to the network in order to optimize it for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section we import all the python packages used in this exercise\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from nilearn.image import resample_img\n",
    "from random import shuffle\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Module, Conv3d, Parameter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(551)\n",
    "torch.cuda.manual_seed_all(551)\n",
    "np.random.seed(551)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "This function is in charge of creating a dataset by scanning the specified path for images and labels and filter the results in order to take only nifty files (extension nii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decathlon_prostate_dataset(images_path, labels_path):\n",
    "    # in this method we create a dataset which is a list of data dictionaries\n",
    "    dataset = []\n",
    "        \n",
    "    file_list = [f for f in os.listdir(images_path) if 'nii' in f]\n",
    "        \n",
    "    for file in file_list:\n",
    "        dataset.append({\n",
    "            'images': [os.path.join(images_path, file)],\n",
    "            'labels': [os.path.join(labels_path, file)],\n",
    "        })\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation and management\n",
    "Here we define a few transformations that allow us to manipulate data in order to feed it to the network and realize training. These transforms, once instantiated, can be chained together. All of them can be called by feeding a data dictionary as input and return a modified version of such dictionary. \n",
    "\n",
    "In this exercise we start out with a dataset of MRI prostate images that contains *file names* for images and labels and we execute the following transformations to obtain images that can be fed to the network.\n",
    "\n",
    "* **LoadNiftyFromFilename** loads nifty (2D)/3D/4D+ data into memory as a nifty image (through nibabel package)\n",
    "* **ResampleNiftyVolume** makes all nifty volumes have the same spatial resolution (mm per voxel)\n",
    "* **NiftyToNumpy** transforms the nifty volumes to numpy\n",
    "* **CropCenteredSubVolume** takes numpy volumes of any spatial size (width x height x depth) and crops them to be a specific size (so they can be fed to the network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadNiftyFromFilename(object):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "\n",
    "    def __call__(self, data):\n",
    "        entries = []\n",
    "\n",
    "        for entry in data[self.field]:\n",
    "            entries.append(nib.load(entry))\n",
    "\n",
    "        data[self.field] = entries\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResampleNiftyVolume(object):\n",
    "    def __init__(self, resolution, field, interpolation='continuous'):\n",
    "        assert len(resolution) == 3\n",
    "        self.interpolation = interpolation\n",
    "        self.resolution = resolution\n",
    "        self.field = field\n",
    "\n",
    "    def __call__(self, data):\n",
    "        entries = []\n",
    "        spacings = []\n",
    "        sizes = []\n",
    "\n",
    "        for entry in data[self.field]:\n",
    "            current_spacing = entry.header.get_zooms()\n",
    "            current_shape = entry.header.get_data_shape()\n",
    "\n",
    "            image_t = resample_img(\n",
    "                img=entry,\n",
    "                target_affine=np.diag([self.resolution[0], self.resolution[1], self.resolution[2]]),\n",
    "                interpolation=self.interpolation\n",
    "            )\n",
    "\n",
    "            entries.append(image_t)\n",
    "            spacings.append(current_spacing)\n",
    "            sizes.append(current_shape)\n",
    "\n",
    "        data[self.field] = entries\n",
    "        data[self.field + '_spacings'] = spacings\n",
    "        data[self.field + '_sizes'] = sizes\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftyToNumpy(object):\n",
    "    def __init__(self, field):\n",
    "        self.field = field\n",
    "\n",
    "    def __call__(self, data):\n",
    "        entries = []\n",
    "\n",
    "        for entry in data[self.field]:\n",
    "            entry_t = entry.get_data().astype(np.float32)\n",
    "            \n",
    "            if entry_t.ndim < 4:  # if label (labels have 3 spatial dimensions (single channel))\n",
    "                # make it single class!!\n",
    "                entry_t = (entry_t > 0.5).astype(np.float32)  \n",
    "                \n",
    "                # add channel dimension (1)\n",
    "                entry_t = entry_t[np.newaxis]\n",
    "            elif entry_t.ndim == 4:  # if image (images have 4 spatial dimensions (two channels))\n",
    "                entry_t = np.transpose(entry_t, [3, 0, 1, 2])\n",
    "                \n",
    "                for i in range(entry_t.shape[0]):\n",
    "                    # normalize channel-wise (each channel is a different MRI pulse sequence, \"color\" MRI)\n",
    "                    entry_t[i] = (entry_t[i] - np.min(entry_t[i])) / (np.max(entry_t[i]) - np.min(entry_t[i]))\n",
    "\n",
    "            entries.append(entry_t)\n",
    "\n",
    "        data[self.field] = entries\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this transformation pads or crops images to make all of them have the same size. \n",
    "# it's a complex transform and you don't need to know all the details about it\n",
    "class CropCenteredSubVolume(object):\n",
    "    def __init__(self, size, image_field, label_field=None):\n",
    "        self.size = size\n",
    "        self.image_field = image_field\n",
    "        self.label_field = label_field\n",
    "        \n",
    "    def pad_to_minimal_size(self, image, pad_mode='constant'):\n",
    "        pad = self.size - np.asarray(image.shape[1:4]) + 1\n",
    "        pad[pad < 0] = 0\n",
    "\n",
    "        pad_before = np.floor(pad / 2.).astype(int)\n",
    "        pad_after = (pad - pad_before).astype(int)\n",
    "\n",
    "        pad_vector = [(0, 0)]\n",
    "        for i in range(image.ndim - 1):\n",
    "            if i < 3:\n",
    "                pad_vector.append((pad_before[i], pad_after[i]))\n",
    "            else:\n",
    "                pad_vector.append((0, 0))\n",
    "        image = np.pad(array=image, pad_width=pad_vector, mode=pad_mode)\n",
    "\n",
    "        return image, pad_before, pad_after\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image_entries = []\n",
    "        label_entries = []\n",
    "\n",
    "        image_field = self.image_field\n",
    "        label_field = self.label_field\n",
    "\n",
    "        for image_entry, label_entry in zip(data[image_field], data[label_field]):\n",
    "            assert np.all(np.asarray(image_entry.shape[1:4]) == np.asarray(label_entry.shape[1:4]))\n",
    "\n",
    "            image_entry, pad_before, pad_after = self.pad_to_minimal_size(image_entry, pad_mode='constant')\n",
    "            label_entry, _, _ = self.pad_to_minimal_size(label_entry, pad_mode='constant')\n",
    "\n",
    "            h_size = np.floor(np.asarray(self.size) / 2.).astype(int)\n",
    "            centr_pix = np.floor(np.asarray(image_entry.shape[1:4]) / 2.).astype(int)\n",
    "\n",
    "            start_px = (centr_pix - h_size).astype(int)\n",
    "\n",
    "            end_px = (start_px + self.size).astype(int)\n",
    "\n",
    "            assert np.all(end_px <= np.asarray(image_entry.shape[1:4]))\n",
    "            assert np.all(start_px >= 0)\n",
    "\n",
    "            image_patch = image_entry[:, start_px[0]:end_px[0], start_px[1]:end_px[1], start_px[2]:end_px[2]]\n",
    "\n",
    "            label_patch = label_entry[:, start_px[0]:end_px[0], start_px[1]:end_px[1], start_px[2]:end_px[2]]\n",
    "\n",
    "            crop_before = start_px\n",
    "            crop_after = image_entry.shape[1:4] - end_px - 1\n",
    "\n",
    "            assert np.all(np.asarray(image_patch.shape[1:4]) == self.size)\n",
    "            assert np.all(np.asarray(label_patch.shape[1:4]) == self.size)\n",
    "\n",
    "            image_entries.append(image_patch)\n",
    "            label_entries.append(label_patch)\n",
    "\n",
    "        data[self.image_field] = image_entries\n",
    "        data[self.label_field] = label_entries\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and iterating\n",
    "\n",
    "In this exercise the batch iterator takes care of the whole data loading/transformation/batching process. It gives us the ability to iterate through the dataset and specify the transformations that need to be applied to the data. All the transformations applied in this exercise are done upon instatiation of the batch iterator. The dataset, which consists of file names at the beginning, get converted to 4D (3D + channel) numpy array having the necessary format and characteristics to be meaningfully used by the network.\n",
    "\n",
    "We implement here such object which has a method `__iter__` allowing us to use it as a generator (Eg. we can write `for batch in iterator: ...`).\n",
    "\n",
    "When executing `for batch in training_batch_iterator:` later in this exercise we will loop through the dataset and obtain batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchIterator(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        keys,\n",
    "        data,\n",
    "        global_transforms,\n",
    "        shuffle=False\n",
    "    ):\n",
    "        self.data = copy.deepcopy(data)\n",
    "        self.keys = keys\n",
    "        self.length = len(data)\n",
    "        self.batch_size = batch_size\n",
    "        self.global_transforms = []\n",
    "        self.n_batches = int(np.ceil(len(data) / self.batch_size))\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.global_transforms = global_transforms\n",
    "        \n",
    "        transform_helper = partial(self.transform_helper, data=self.data, transforms=self.global_transforms)\n",
    "\n",
    "        with ThreadPool(32) as p:\n",
    "            p.map(transform_helper, range(len(self.data)))\n",
    "                \n",
    "    @staticmethod\n",
    "    def transform_helper(idx, data, transforms):\n",
    "        for transform in transforms:\n",
    "            data[idx] = transform(data[idx])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = copy.deepcopy(self.data)\n",
    "\n",
    "        if self.shuffle:\n",
    "            shuffle(data)\n",
    "\n",
    "        for i in range(self.n_batches):\n",
    "            curr_data = data[i * self.batch_size:np.min([(i+1) * self.batch_size, self.length])]\n",
    "\n",
    "            # collate batch (from a list of dictionary to a dictionary of lists)\n",
    "            batch = {}\n",
    "            for key in self.keys:\n",
    "                batch[key] = []\n",
    "                for j in range(len(curr_data)):\n",
    "                    batch[key].append(curr_data[j][key][0])\n",
    "                batch[key] = np.stack(batch[key])\n",
    "\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/validation parameters\n",
    "Here we specify all the hyper-parameters and options that define what/how our network will learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFININING MODEL PARAMETERS\n",
    "\n",
    "# voxel size of the image (3d images)\n",
    "image_size = [256, 256, 96]\n",
    "# resolution of each voxel in millimeters\n",
    "image_resolution = [1, 1, 1]\n",
    "\n",
    "# batch size for iterator (how many images get fed to network at each batch)\n",
    "batch_size = 2\n",
    "# batch size for learning (every how many batches should we backpropagate) \n",
    "# (Setting the previous parameter to 2 and this to 4 induces a behavior similar to batch size 8) \n",
    "effective_batchsize = 1 \n",
    "# normalization ('none'|'batchnorm'|'groupnorm')\n",
    "normalization = 'groupnorm'\n",
    "# number of training epochs\n",
    "num_epochs = 50\n",
    "# learning rate\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# path of the datasets for training and validation\n",
    "training_images_path = './Task05_Prostate/imagesTr'\n",
    "training_labels_path = './Task05_Prostate/labelsTr'\n",
    "\n",
    "# [todo] change validation path\n",
    "validation_images_path = './Task05_Prostate/imagesVd'\n",
    "validation_labels_path = './Task05_Prostate/labelsVd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and transformations\n",
    "Here we define the sequence of transformations we do over our dataset. First we load our images and labels, then we resample them to make them have the same voxel resolution across cases, we then convert them to numpy format and finally crop them to have the same voxel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING DATA TRANSFORMATION SEQUENCE. TRANSFORMS DEFINED ABOVE\n",
    "\n",
    "global_transforms = [\n",
    "    LoadNiftyFromFilename('images'),\n",
    "    LoadNiftyFromFilename('labels'),\n",
    "    ResampleNiftyVolume(field='images', resolution=image_resolution, interpolation='continuous'),\n",
    "    ResampleNiftyVolume(field='labels', resolution=image_resolution, interpolation='nearest'),\n",
    "    NiftyToNumpy('images'),\n",
    "    NiftyToNumpy('labels'),\n",
    "    CropCenteredSubVolume(size=image_size, image_field='images', label_field='labels'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate here 1) datasets and 2) batch iterators which provide training and validation batches from our dataset to the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASET FOR TRAINING AND VALIDATION -- LOADING WILL TAKE TIME -- LOAD ALL DATA IN MEMORY --\n",
    "\n",
    "training_dataset = make_decathlon_prostate_dataset(training_images_path, training_labels_path)\n",
    "\n",
    "validation_dataset = make_decathlon_prostate_dataset(validation_images_path, validation_labels_path)\n",
    "\n",
    "# TRAINING BATCH ITERATOR\n",
    "train_batch_iterator = BatchIterator(\n",
    "            batch_size=batch_size,\n",
    "            keys=['images', 'labels'],\n",
    "            data=training_dataset,\n",
    "            global_transforms=global_transforms,\n",
    "            shuffle=True\n",
    ")\n",
    "\n",
    "# VALIDATION BATCH ITERATOR\n",
    "valid_batch_iterator = BatchIterator(\n",
    "            batch_size=batch_size,\n",
    "            keys=['images', 'labels'],\n",
    "            data=validation_dataset,\n",
    "            global_transforms=global_transforms,\n",
    "            shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at your data\n",
    "It is important to always inspect the data before deciding what method is appropriate to solve the problem at hand. Having a look at the data might mean to take into consideration statistics and distributions underlying the dataset, but in this case we are interested in visually inspecting it in order to be sure that it has been loaded and transformed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "for data in train_batch_iterator:\n",
    "    # showing just 1 channel out of two from MRI image\n",
    "    # label has been thresholded to be class 0 background, class 1 prostate\n",
    "    plt.imshow(np.squeeze(data['images'][0, 0, :, : , 30] + data['labels'][0, 0, :, : , 30]))  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:green\">V-Net network architecture</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BatchNorm / GroupNorm\n",
    "\n",
    "Normalizing the data as it passes through the network provides two main advantages. First, it keeps the values during training centered near 0, which is where activation functions are most nonlinear and hence most sensitive. Second, it introduces some noise, as the renormalizing procedure varies between images and discards a small amount of information. The addition of noise makes the network more robust and less likely to overfit.\n",
    "\n",
    "However, there are multiple types of normalization available. A conventional choice is BatchNorm. In this method, for a single channel (the output of a particular convolution kernel from a previous layer), the mean and standard deviation of all pixel values in the channel are computed across an entire batch. The mean is subtracted from each pixel value, centering them at zero, and then they are scaled by the standard deviation, giving the new data mean 0 and standard deviation 1. Finally, every value is re-scaled by a single, trainable weight, and shifted by a trainable bias. For example, in a batch of 20 100x100 RGB images, the 20x100x100 = 200,000 red pixel values would be grouped together, as would the 200,000 green and blue values, respectively.\n",
    "\n",
    "The disadvantage of this method is that when we deploy our model, the statistics of a single image are non-existent, so a running average must be computed during training. Similarly, if the model or input is large, the batch size may be small, again reducing the statistical power.\n",
    "\n",
    "A solution to these disadvantages is found in the GroupNorm. The procedure is quite similar, but the statistics are only computed for a single image at a time. To improve statistical power, multiple channels are grouped together instead. This leaves us without the problems on small batches or at deployment. In the example above, the RGB images of size 100x100, regardless of batch size, 3x100x100 = 30,000 values would be normalized together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice loss/score formulation\n",
    "We define here the dice loss layer which penalizes segmentations that do not overlap very well with the ground truth. As explained before this overlap measure between two (binary) contours has been generalized and introduced as an objective function for FCNNs in [Ref. 3]. The formulation used in this work is DICE=2 * (Gt * Pred) / (Gt^2 + Pred^2).\n",
    "\n",
    "We provide here a PyTorch implementation of said Dice loss layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 0.00001\n",
    "\n",
    "class DiceLoss(Module):\n",
    "    def forward(self, input, target):\n",
    "        num = (input * target).sum(dim=4, keepdim=True).sum(dim=3, keepdim=True).sum(dim=2, keepdim=True)\n",
    "        den1 = input.pow(2).sum(dim=4, keepdim=True).sum(dim=3, keepdim=True).sum(dim=2, keepdim=True)\n",
    "        den2 = target.pow(2).sum(dim=4, keepdim=True).sum(dim=3, keepdim=True).sum(dim=2, keepdim=True)\n",
    "\n",
    "        dice = (2.0 * num / (den1 + den2 + EPS))\n",
    "        \n",
    "        return (1.0 - dice).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network architecture\n",
    "In this exercise we will use a network **similar** to V-Net, which was introduced in [Ref. 2]. Although we introduced some changes (BatchNorm/GroupNorm, summation for skip connection and different input size) to make this exercise more actual and general, the network architecture remains slightly the same. A schematic representation of this architecture is shown below.\n",
    "\n",
    "![V-Net](diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch implementation\n",
    "A network inspired by V-Net, and further improving it, has been implmented here in a slightly different manner than the original paper.\n",
    "\n",
    "Differently than original V-Net we propose to:\n",
    "* Allow user to specify if batchnorm or groupnorm should be used\n",
    "* Use a summation strategy for long skip connections instead of concatenation\n",
    "* Sligthly change the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm3D(Module):\n",
    "    def __init__(self, num_features, num_groups=16, eps=1e-5):\n",
    "        super(GroupNorm3D, self).__init__()\n",
    "        self.weight = Parameter(torch.ones(1, num_features, 1, 1, 1))\n",
    "        self.bias = Parameter(torch.zeros(1, num_features, 1, 1, 1))\n",
    "        self.num_groups = num_groups\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W, D = x.size()\n",
    "        G = self.num_groups\n",
    "        assert C % G == 0\n",
    "\n",
    "        x = x.view(N, G, -1)\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True)\n",
    "\n",
    "        x = (x-mean) / (var+self.eps).sqrt()\n",
    "        x = x.view(N, C, H, W, D)\n",
    "        return x * self.weight + self.bias\n",
    "    \n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, n_stages, n_filters_in, n_filters_out, normalization='none', expand_chan=False):\n",
    "        super(ResidualConvBlock, self).__init__()\n",
    "\n",
    "        self.expand_chan = expand_chan\n",
    "        if self.expand_chan:\n",
    "            ops = []\n",
    "\n",
    "            ops.append(nn.Conv3d(n_filters_in, n_filters_out, 1))\n",
    "\n",
    "            if normalization == 'batchnorm':\n",
    "                ops.append(nn.BatchNorm3d(n_filters_out))\n",
    "            if normalization == 'groupnorm':\n",
    "                ops.append(GroupNorm3D(n_filters_out))\n",
    "\n",
    "            ops.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            self.conv_expan = nn.Sequential(*ops)\n",
    "\n",
    "        ops = []\n",
    "        for i in range(n_stages):\n",
    "            if normalization != 'none':\n",
    "                ops.append(nn.Conv3d(n_filters_in, n_filters_out, 3, padding=1))\n",
    "                if normalization == 'batchnorm':\n",
    "                    ops.append(nn.BatchNorm3d(n_filters_out))\n",
    "                if normalization == 'groupnorm':\n",
    "                    ops.append(GroupNorm3D(n_filters_out))\n",
    "            else:\n",
    "                ops.append(nn.Conv3d(n_filters_in, n_filters_out, 3, padding=1))\n",
    "\n",
    "            ops.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv = nn.Sequential(*ops)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.expand_chan:\n",
    "            x = self.conv(x) + self.conv_expan(x)\n",
    "        else:\n",
    "            x = (self.conv(x) + x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownsamplingConvBlock(nn.Module):\n",
    "    def __init__(self, n_filters_in, n_filters_out, stride=2, normalization='none'):\n",
    "        super(DownsamplingConvBlock, self).__init__()\n",
    "\n",
    "        ops = []\n",
    "        if normalization != 'none':\n",
    "            ops.append(nn.Conv3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
    "            if normalization == 'batchnorm':\n",
    "                ops.append(nn.BatchNorm3d(n_filters_out))\n",
    "            if normalization == 'groupnorm':\n",
    "                ops.append(GroupNorm3D(n_filters_out))\n",
    "        else:\n",
    "            ops.append(nn.Conv3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
    "\n",
    "        ops.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv = nn.Sequential(*ops)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpsamplingDeconvBlock(nn.Module):\n",
    "    def __init__(self, n_filters_in, n_filters_out, stride=2, normalization='none'):\n",
    "        super(UpsamplingDeconvBlock, self).__init__()\n",
    "\n",
    "        ops = []\n",
    "        if normalization != 'none':\n",
    "            ops.append(nn.ConvTranspose3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
    "            if normalization == 'batchnorm':\n",
    "                ops.append(nn.BatchNorm3d(n_filters_out))\n",
    "            if normalization == 'groupnorm':\n",
    "                ops.append(GroupNorm3D(n_filters_out))\n",
    "        else:\n",
    "            ops.append(nn.ConvTranspose3d(n_filters_in, n_filters_out, stride, padding=0, stride=stride))\n",
    "\n",
    "        ops.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        self.conv = nn.Sequential(*ops)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, n_filters=16, normalization='none'):\n",
    "        super(VNet, self).__init__()\n",
    "\n",
    "        if n_channels > 1:\n",
    "            self.block_one = ResidualConvBlock(1, n_channels, n_filters, normalization=normalization, expand_chan=True)\n",
    "        else:\n",
    "            self.block_one = ResidualConvBlock(1, n_channels, n_filters, normalization=normalization)\n",
    "\n",
    "        self.block_one_dw = DownsamplingConvBlock(n_filters, 2 * n_filters, normalization=normalization)\n",
    "\n",
    "        self.block_two = ResidualConvBlock(2, n_filters * 2, n_filters * 2, normalization=normalization)\n",
    "        self.block_two_dw = DownsamplingConvBlock(n_filters * 2, n_filters * 4, normalization=normalization)\n",
    "\n",
    "        self.block_three = ResidualConvBlock(3, n_filters * 4, n_filters * 4, normalization=normalization)\n",
    "        self.block_three_dw = DownsamplingConvBlock(n_filters * 4, n_filters * 8, normalization=normalization)\n",
    "\n",
    "        self.block_four = ResidualConvBlock(3, n_filters * 8, n_filters * 8, normalization=normalization)\n",
    "        self.block_four_dw = DownsamplingConvBlock(n_filters * 8, n_filters * 16, normalization=normalization)\n",
    "\n",
    "        self.block_five = ResidualConvBlock(3, n_filters * 16, n_filters * 16, normalization=normalization)\n",
    "        self.block_five_up = UpsamplingDeconvBlock(n_filters * 16, n_filters * 8, normalization=normalization)\n",
    "\n",
    "        self.block_six = ResidualConvBlock(3, n_filters * 8, n_filters * 8, normalization=normalization)\n",
    "        self.block_six_up = UpsamplingDeconvBlock(n_filters * 8, n_filters * 4, normalization=normalization)\n",
    "\n",
    "        self.block_seven = ResidualConvBlock(3, n_filters * 4, n_filters * 4, normalization=normalization)\n",
    "        self.block_seven_up = UpsamplingDeconvBlock(n_filters * 4, n_filters * 2, normalization=normalization)\n",
    "\n",
    "        self.block_eight = ResidualConvBlock(2, n_filters * 2, n_filters * 2, normalization=normalization)\n",
    "        self.block_eight_up = UpsamplingDeconvBlock(n_filters * 2, n_filters, normalization=normalization)\n",
    "\n",
    "        self.block_nine = ResidualConvBlock(1, n_filters, n_filters, normalization=normalization)\n",
    "\n",
    "        self.out_conv = nn.Conv3d(n_filters, n_classes, 1, padding=0)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x1 = self.block_one(input)\n",
    "        x1_dw = self.block_one_dw(x1)\n",
    "\n",
    "        x2 = self.block_two(x1_dw)\n",
    "        x2_dw = self.block_two_dw(x2)\n",
    "\n",
    "        x3 = self.block_three(x2_dw)\n",
    "        x3_dw = self.block_three_dw(x3)\n",
    "\n",
    "        x4 = self.block_four(x3_dw)\n",
    "        x4_dw = self.block_four_dw(x4)\n",
    "\n",
    "        x5 = self.block_five(x4_dw)\n",
    "\n",
    "        x5_up = self.block_five_up(x5)\n",
    "        x5_up = x5_up + x4\n",
    "\n",
    "        x6 = self.block_six(x5_up)\n",
    "        x6_up = self.block_six_up(x6)\n",
    "        x6_up = x6_up + x3\n",
    "\n",
    "        x7 = self.block_seven(x6_up)\n",
    "        x7_up = self.block_seven_up(x7)\n",
    "        x7_up = x7_up + x2\n",
    "\n",
    "        x8 = self.block_eight(x7_up)\n",
    "        x8_up = self.block_eight_up(x8)\n",
    "        x8_up = x8_up + x1\n",
    "\n",
    "        x9 = self.block_nine(x8_up)\n",
    "\n",
    "        out = self.out_conv(x9)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_seg = VNet(n_channels=2, n_classes=1, n_filters=16, normalization=normalization)\n",
    "net_seg = torch.nn.DataParallel(net_seg)\n",
    "net_seg.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of training + validation loops\n",
    "We implement here the training and validation loops. The training loop implements a cycle over the batches that can be obtained from the training set and feeds them to the network. After the forward propagation, gradients are backpropagated so that the parameters of the networks can be updated and training can take place. One thing that is very convenient in PyTorch is the ability to update the parameters of the network once every multiple batches. This is particularly useful in situation, such as the current one, where due to GPU memory constraints it is not possible to use large batch sizes during training (here we use batch size two, resulting in almost full usage of 16 GB of GPU memory).\n",
    "\n",
    "The validation loop feeds data to the network and executes the forward propagation step. With the directive `net_seg.eval()` we instruct pytorch to put the network in evaluation mode (as opposed to training mode) and `with torch.no_grad():` we are asking pytorch not to allocate memory for gradient as we won't need to compute any during validation.\n",
    "\n",
    "Together, the two loops implement training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(net_seg):\n",
    "    optimizer_seg = optim.Adam(net_seg.parameters(), lr=learning_rate)\n",
    "    criterion_seg = DiceLoss()\n",
    "    \n",
    "    running_seg_loss = 0.0\n",
    "        \n",
    "    for i, data in enumerate(train_batch_iterator, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = torch.from_numpy(data['images']), torch.from_numpy(data['labels'])\n",
    "\n",
    "        if torch.cuda.is_available:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        if (i % effective_batchsize) == 0:\n",
    "            # zero the parameter gradients\n",
    "            optimizer_seg.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs_seg = net_seg(inputs)\n",
    "\n",
    "        outputs_seg = torch.nn.Sigmoid()(outputs_seg)\n",
    "\n",
    "        loss_seg = criterion_seg(outputs_seg, labels)\n",
    "\n",
    "        loss_seg.backward()\n",
    "        optimizer_seg.step()\n",
    "\n",
    "        running_seg_loss += loss_seg.detach().cpu().item()\n",
    "        \n",
    "    avg_loss = running_seg_loss / len(train_batch_iterator)\n",
    "    one_output_seg = outputs_seg.detach().cpu().numpy()[0]\n",
    "    one_output_img = inputs.detach().cpu().numpy()[0]\n",
    "        \n",
    "    return avg_loss, one_output_seg, one_output_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(net_seg):\n",
    "    net_seg.eval()\n",
    "    criterion_seg = DiceLoss()\n",
    "    running_seg_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valid_batch_iterator, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = torch.from_numpy(data['images']), torch.from_numpy((data['labels']))\n",
    "\n",
    "            if torch.cuda.is_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs_seg = net_seg(inputs)\n",
    "\n",
    "            outputs_seg = torch.nn.Sigmoid()(outputs_seg)\n",
    "\n",
    "            loss_seg = criterion_seg(outputs_seg, labels)\n",
    "\n",
    "            running_seg_loss += loss_seg.detach().cpu().item()\n",
    "            \n",
    "    avg_loss = running_seg_loss / len(valid_batch_iterator)\n",
    "    one_output_seg = outputs_seg.detach().cpu().numpy()[0]\n",
    "    one_output_img = inputs.detach().cpu().numpy()[0]\n",
    "        \n",
    "    return avg_loss, one_output_seg, one_output_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We run training + validation by executing this cell\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "print('STARTING TRAINING')\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    train_loss, train_output_seg, train_input_img = run_training(net_seg)\n",
    "    valid_loss, valid_output_seg, valid_input_img = run_validation(net_seg)\n",
    "    print('EPOCH {} of {}'.format(i, num_epochs))\n",
    "    print('-- train loss {} -- valid loss {} --'.format(train_loss, valid_loss))\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    validation_losses.append(valid_loss)\n",
    "    \n",
    "    plt.imshow(np.squeeze(train_input_img[0, :, : , 30] + train_output_seg[0, :, : , 30]))  # showing just 1 channel out of two from MRI image\n",
    "    plt.show()\n",
    "    plt.imshow(np.squeeze(valid_input_img[0, :, : , 30] + valid_output_seg[0, :, : , 30]))  # label has been thresholded to be class 0 background, class 1 prostate\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(range(len(train_losses)), train_losses, 'b', range(len(validation_losses)), validation_losses, 'r')\n",
    "    red_patch = mpatches.Patch(color='red', label='Validation')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Training')\n",
    "    plt.legend(handles=[red_patch, blue_patch])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting scores\n",
    "We now plot the training and validation scores in terms of Dice loss (the lower, the better). More informations about the results and more experiments can be found in the original paper [Ref. 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_losses)), train_losses, 'b', range(len(validation_losses)), validation_losses, 'r')\n",
    "red_patch = mpatches.Patch(color='red', label='Validation')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Training')\n",
    "plt.legend(handles=[red_patch, blue_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try your own experiment!\n",
    "What happens when you change the normalization strategy (when you declare net_seg) to `batchnorm` or `none`? What about other changes such as learning rate? What happens when you increase the effective batch_size by varying the `effective_batchsize` parameter? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">Conclusions</span>\n",
    "In this lab you have seen how to implement a network architecture **similar** to V-Net. V-Net is one of the first works proposing 3D segmentation and is the first work that introduced Dice loss as an objective function that can be optimized to deliver superior segmentation performance. Dice loss is the objective function of choice for medical image segmentation and has been undes by hundreds of groups worldwide. Network architecture derived from V-Net and further improving it have been also developed and have been applied to a multitude of tasks and a wide range of anatomies. \n",
    "\n",
    "In this lab we have shown the details of how to achieve an improved implementation of V-Net by using modern DL software and advanced python design constructs. By trying your own experiments you should be also able to further explore the effect of specific design choices on the outcome of the experiments and performance of the network. \n",
    "\n",
    "Despite the slow training procedure, due to the high computational load of the task presented in this lab, the network starts showing signs of convergence within 50 epochs. In reality, such a network would need to be trained for much longer periods of time, amounting to days or even weeks (depending on the dataset) on modern GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Ref. 1: *Milletari, F., Navab, N. and Ahmadi, S.A., 2016, October. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3D Vision (3DV), 2016 Fourth International Conference on (pp. 565-571). IEEE.*\n",
    "* Ref. 2: *Ronneberger, O., Fischer, P. and Brox, T., 2015, October. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.*\n",
    "* Ref. 3: *Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167*\n",
    "* Ref. 4: *Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S. and Pal, C., 2016. The importance of skip connections in biomedical image segmentation. In Deep Learning and Data Labeling for Medical Applications (pp. 179-187). Springer, Cham*\n",
    "* Ref. 5: *Milletari, Fausto. Hough Voting Strategies for Segmentation, Detection and Tracking. Diss. Universitt Mnchen, 2018.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
